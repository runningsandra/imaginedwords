{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandra/Documents/Programming/imaginedwords/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../.env\")\n",
    "homedir = os.getenv(\"HOMEDIR\")\n",
    "datadir = os.getenv(\"DATADIR\")\n",
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listfiles = os.listdir(os.path.join(datadir, \"example_files\"))\n",
    "file_path = os.path.join(datadir, \"example_files\", listfiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_prompt = \"\"\"\n",
    "Task:\n",
    "\n",
    "Extract all the text content from the given image and format it in HTML.\n",
    "Your goal is to preserve both the content and layout of the text as accurately as possible.\n",
    "\n",
    "\n",
    "Extraction Rules:\n",
    "\n",
    "    1. General HTML Structure:\n",
    "    Wrap the entire output in a <html> document structure:\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Extracted Text</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <!-- Content goes here -->\n",
    "    </body>\n",
    "    </html>\n",
    "\n",
    "    2. Text Formatting:\n",
    "    Use appropriate HTML tags for text elements:\n",
    "        - Headings: <h1>, <h2>, <h3>, etc., for titles and sections.\n",
    "        - Paragraphs: Wrap regular text in <p> tags.\n",
    "        - Lists:\n",
    "            - Use <ul> and <li> for unordered (bulleted) lists.\n",
    "            - Use <ol> and <li> for ordered (numbered) lists.\n",
    "        - Emphasis:\n",
    "            - Bold text: <strong>\n",
    "            - Italic text: <em>\n",
    "        - Line breaks: Use <br> for single line breaks where necessary.\n",
    "\n",
    "    3. Tables:\n",
    "    Use proper HTML table structure for tabular content:\n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Column 1</th>\n",
    "                <th>Column 2</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>Row 1 Col 1</td>\n",
    "                <td>Row 1 Col 2</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Row 2 Col 1</td>\n",
    "                <td>Row 2 Col 2</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "    4. Layout and Hierarchy Preservation:\n",
    "    - Reproduce the original text structure and hierarchy (headings, subheadings, paragraphs, lists, and tables) as accurately as possible.\n",
    "    - Retain visual structure, such as spacing and breaks, using <div> or <br> when appropriate.\n",
    "    - Use nested HTML tags for lists, sublists, or complex structures.\n",
    "\n",
    "    5. Handling Special Cases:\n",
    "    - If any part of the image text is illegible, replace it with: <span>[illegible text]</span>\n",
    "    - Do not add any additional comments or annotations that are not in the original image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": file_path,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": html_prompt},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=2000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
